{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1) What is Boosting in Machine Learning? Explain how it improves weak learners."
      ],
      "metadata": {
        "id": "uLU7bAYZ0s2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting** is a machine learning ensemble technique that combines multiple **weak learners** to create a **strong learner**. The core idea is to train models sequentially, each one trying to correct the errors of its predecessor.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What is a Weak Learner?\n",
        "\n",
        "A **weak learner** is a model that performs slightly better than random guessing. For example, in binary classification, a weak learner might have an accuracy just over 50%.\n",
        "\n",
        "The most common weak learners used in boosting are **decision stumps**, which are decision trees with only one split.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° How Boosting Works (Step-by-Step):\n",
        "\n",
        "1. **Initialize Weights**:\n",
        "\n",
        "   * Assign equal weights to all training examples.\n",
        "\n",
        "2. **Train First Weak Learner**:\n",
        "\n",
        "   * A model is trained on the data with the current weights.\n",
        "\n",
        "3. **Evaluate Errors**:\n",
        "\n",
        "   * Misclassified points get higher weights, while correctly classified points get lower weights.\n",
        "\n",
        "4. **Train Next Weak Learner**:\n",
        "\n",
        "   * The next model focuses more on the hard-to-classify examples (those with higher weights).\n",
        "\n",
        "5. **Repeat**:\n",
        "\n",
        "   * This process continues for a fixed number of iterations or until the model achieves good performance.\n",
        "\n",
        "6. **Combine Learners**:\n",
        "\n",
        "   * Final prediction is made by combining all weak learners using a weighted vote or sum.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why Does It Work?\n",
        "\n",
        "Boosting improves performance by:\n",
        "\n",
        "* Focusing sequentially on the mistakes made by previous learners.\n",
        "* Reducing **bias** and **variance**.\n",
        "* Converting weak learners into a strong ensemble that generalizes well.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Popular Boosting Algorithms\n",
        "\n",
        "* **AdaBoost (Adaptive Boosting)** ‚Äì Changes weights of instances.\n",
        "* **Gradient Boosting** ‚Äì Optimizes loss function using gradient descent.\n",
        "* **XGBoost**, **LightGBM**, **CatBoost** ‚Äì Efficient implementations for large-scale applications.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Example Use Case\n",
        "\n",
        "Suppose we want to detect spam emails. A single weak learner might just look at the presence of the word \"free\". Not enough to classify well.\n",
        "\n",
        "With boosting:\n",
        "\n",
        "* First learner classifies based on \"free\".\n",
        "* Second learner focuses on misclassified emails, maybe learns to look for \"urgent\".\n",
        "* Third learner adds \"win now\", and so on.\n",
        "* The ensemble of all these simple models can accurately detect spam.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "* **Boosting** is about combining many weak learners to create a strong learner.\n",
        "* It does this by training models sequentially and focusing on errors.\n",
        "* It's powerful and widely used for structured/tabular data tasks.\n",
        "\n",
        "Let me know if you'd like a visual explanation or code example.\n"
      ],
      "metadata": {
        "id": "tT_aIIOT0uRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?"
      ],
      "metadata": {
        "id": "PKvY-Mp80uOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between **AdaBoost** and **Gradient Boosting** lies in **how** they train models and update them during the boosting process.\n",
        "\n",
        "Here‚Äôs a clear comparison:\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ **1. Training Approach**\n",
        "\n",
        "| Feature              | **AdaBoost**                                                   | **Gradient Boosting**                                                              |\n",
        "| -------------------- | -------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
        "| **Focus**            | Adjusts **weights of data points**                             | Optimizes a **loss function** using gradients                                      |\n",
        "| **Error Handling**   | Increases weights of **misclassified samples**                 | Fits the next model to the **residual errors** (negative gradient)                 |\n",
        "| **Learner Training** | Each learner focuses more on previously **misclassified data** | Each learner **corrects the residuals** (difference between prediction and actual) |\n",
        "| **Update Mechanism** | Instance weights updated after each learner                    | Residuals calculated and new learner trained on them                               |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **2. Model Output Combination**\n",
        "\n",
        "| Aspect          | AdaBoost                                                             | Gradient Boosting                                                |\n",
        "| --------------- | -------------------------------------------------------------------- | ---------------------------------------------------------------- |\n",
        "| **Combination** | Weighted majority vote (classification) or weighted sum (regression) | Sum of all models‚Äô outputs (usually added to current prediction) |\n",
        "| **Weighting**   | Each model‚Äôs vote depends on its accuracy                            | Each model is scaled by a **learning rate**                      |\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Example:\n",
        "\n",
        "* **AdaBoost**:\n",
        "\n",
        "  * Starts with equal weights.\n",
        "  * Misclassified points get higher weights.\n",
        "  * Next learner pays more attention to those.\n",
        "  * Final result = weighted vote of all learners.\n",
        "\n",
        "* **Gradient Boosting**:\n",
        "\n",
        "  * Starts with an initial prediction (e.g., mean).\n",
        "  * Calculates residuals (errors).\n",
        "  * Trains next model to predict these residuals.\n",
        "  * Updates the overall prediction.\n",
        "  * Final result = sum of all learners' contributions.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Summary:\n",
        "\n",
        "| Feature                 | **AdaBoost**                 | **Gradient Boosting**                  |\n",
        "| ----------------------- | ---------------------------- | -------------------------------------- |\n",
        "| **Weighting mechanism** | Adjusts data point weights   | Fits model to gradients (residuals)    |\n",
        "| **Loss optimization**   | Not explicit                 | Directly minimizes a loss function     |\n",
        "| **Interpretation**      | Focus on hard examples       | Focus on reducing loss                 |\n",
        "| **Flexibility**         | Primarily for classification | Used for classification and regression |\n"
      ],
      "metadata": {
        "id": "myfMMNBX0uLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "YHbyZ-Aj0uFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **XGBoost** (Extreme Gradient Boosting), **regularization** plays a crucial role in improving model performance and preventing overfitting. It helps to control the complexity of the model by penalizing overly complex trees, which could otherwise result in poor generalization to new data.\n",
        "\n",
        "### **Key Regularization Techniques in XGBoost**\n",
        "\n",
        "1. **L1 Regularization (Lasso)**:\n",
        "\n",
        "   * L1 regularization penalizes the **absolute values** of the weights (coefficients) of the model.\n",
        "   * It helps in **feature selection** by encouraging sparsity, meaning that some feature weights are reduced to zero, effectively removing those features from the model.\n",
        "   * This is controlled by the parameter `alpha` in XGBoost.\n",
        "\n",
        "2. **L2 Regularization (Ridge)**:\n",
        "\n",
        "   * L2 regularization penalizes the **squared values** of the weights, thus keeping them smaller and more evenly distributed.\n",
        "   * It helps in avoiding large weights and reducing model complexity.\n",
        "   * In XGBoost, this is controlled by the `lambda` parameter.\n",
        "\n",
        "3. **Tree Complexity Control**:\n",
        "   XGBoost also controls the complexity of the individual trees, which is another form of regularization:\n",
        "\n",
        "   * **Max Depth (`max_depth`)**: Controls the depth of each tree. Limiting the depth prevents the tree from growing too large and capturing noise in the data.\n",
        "   * **Minimum Child Weight (`min_child_weight`)**: Specifies the minimum sum of instance weights (or number of samples) required in a child node. This helps control the tree‚Äôs growth.\n",
        "   * **Subsampling**: XGBoost can randomly sample a fraction of the training data (`subsample`) to build each tree, which prevents overfitting by ensuring trees don't overfit to specific patterns in the training set.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Regularization Helps in XGBoost**:\n",
        "\n",
        "1. **Prevents Overfitting**: Regularization penalizes overly complex models, preventing them from fitting noise in the data.\n",
        "2. **Improves Generalization**: By controlling the complexity of individual trees and shrinking feature weights, regularization helps the model generalize better to unseen data.\n",
        "3. **Feature Selection**: L1 regularization encourages sparse solutions, effectively removing irrelevant or redundant features, which improves model interpretability and efficiency.\n",
        "4. **Balances Bias-Variance Tradeoff**: The regularization terms help balance the tradeoff between bias (underfitting) and variance (overfitting), allowing the model to achieve a sweet spot for prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **XGBoost Regularization Formula**:\n",
        "\n",
        "The objective function in XGBoost, which is minimized during training, consists of two parts:\n",
        "\n",
        "1. **Loss Function** (typically Mean Squared Error or Log-Loss for regression and classification)\n",
        "2. **Regularization Term** (penalizes model complexity)\n",
        "\n",
        "So, the overall objective function can be expressed as:\n",
        "\n",
        "$$\n",
        "\\text{Objective} = \\text{Loss Function} + \\text{Regularization Term}\n",
        "$$\n",
        "\n",
        "The regularization term in XGBoost is given by:\n",
        "\n",
        "$$\n",
        "\\text{Regularization} = \\sum_{k} \\left[ \\lambda \\cdot \\text{Leaf Weight}^2 + \\alpha \\cdot |\\text{Leaf Weight}| + \\gamma \\cdot \\text{Tree Complexity} \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$ (L2 regularization) controls the weight size.\n",
        "* $\\alpha$ (L1 regularization) controls sparsity.\n",
        "* $\\gamma$ controls the minimum number of samples required to form a new tree.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Tune Regularization Parameters in XGBoost**:\n",
        "\n",
        "* **`lambda` (L2)**: Set to a small value (e.g., 0, 1) if you want to prevent overly large weights in trees.\n",
        "* **`alpha` (L1)**: Set to a value greater than 0 to encourage sparsity. A larger alpha value means more aggressive feature selection.\n",
        "* **`max_depth`**: Controls how deep the trees grow. A smaller value may help reduce overfitting.\n",
        "* **`min_child_weight`**: This is often tuned to control how many samples a leaf node should contain before splitting. Larger values prevent overfitting.\n",
        "* **`subsample` and `colsample_bytree`**: Randomly sample a subset of data and features to prevent trees from overfitting on the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of XGBoost Regularization in Code**:\n",
        "\n",
        "```python\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up XGBoost parameters with regularization\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # Regression problem\n",
        "    'booster': 'gbtree',  # Tree-based model\n",
        "    'eta': 0.1,  # Learning rate\n",
        "    'max_depth': 4,  # Depth of each tree\n",
        "    'lambda': 1,  # L2 regularization\n",
        "    'alpha': 0.1,  # L1 regularization\n",
        "    'min_child_weight': 1,  # Minimum sum of instance weight\n",
        "    'subsample': 0.8,  # Subsample ratio of the training data\n",
        "    'colsample_bytree': 0.8,  # Subsample ratio of features for each tree\n",
        "    'n_estimators': 100  # Number of boosting rounds\n",
        "}\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBRegressor(**params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "score = model.score(X_test, y_test)\n",
        "print(f\"R^2 score: {score:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**:\n",
        "\n",
        "Regularization in **XGBoost** helps in:\n",
        "\n",
        "* Reducing **overfitting** by controlling model complexity.\n",
        "* Encouraging **sparsity** and **feature selection** via L1 regularization.\n",
        "* Balancing bias and variance to create a model that generalizes well on unseen data.\n",
        "\n",
        "By fine-tuning regularization parameters, you can create a more robust and interpretable model."
      ],
      "metadata": {
        "id": "8WojWC9g2Xoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "GjtZWX-P2JtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CatBoost** is highly regarded for its ability to efficiently handle **categorical data** without requiring extensive preprocessing like one-hot encoding. The key to its efficiency lies in how it **encodes categorical features**, **builds decision trees**, and applies **gradient boosting**. Here‚Äôs why CatBoost stands out:\n",
        "\n",
        "### 1. **Categorical Feature Handling with Ordered Target Statistics**\n",
        "\n",
        "* **CatBoost** uses a novel method called **ordered target statistics** (also known as **ordered boosting**) to handle categorical features.\n",
        "* Traditional gradient boosting models (like XGBoost or LightGBM) typically convert categorical features into numerical representations using **one-hot encoding** or **integer encoding**. However, these methods don‚Äôt always capture the relationships or distributions of categories effectively, especially when dealing with high-cardinality features (features with many unique categories).\n",
        "* Instead of relying on these basic encoding methods, CatBoost performs **target-based encoding** for categorical features, where the model calculates the **mean target value** for each category based on previous data points in the training process.\n",
        "\n",
        "  * This is done **sequentially** to avoid **data leakage** (using future information to predict the current sample).\n",
        "  * CatBoost maintains a **prior** for each category and updates it based on observed patterns, which leads to better handling of unseen categories during training.\n",
        "\n",
        "### 2. **Ordered Target Statistics to Prevent Overfitting**\n",
        "\n",
        "* **Ordered boosting** ensures that the target statistics are computed in a way that prevents the model from leaking information about the target variable.\n",
        "* For each category, the model computes the mean of the target variable but **excludes** the current sample (i.e., uses only previous data). This process ensures that the computed statistics are not influenced by the current row, mitigating overfitting risks.\n",
        "\n",
        "### 3. **Efficient Handling of High Cardinality Categorical Features**\n",
        "\n",
        "* Categorical features with many possible values (high cardinality) are particularly challenging in other algorithms because they can result in a large number of dummy variables when one-hot encoding is used. This increases the dimensionality of the dataset and often leads to inefficiency.\n",
        "* **CatBoost** handles high-cardinality categorical features more efficiently by calculating a single numeric representation per category rather than creating a large number of binary columns.\n",
        "\n",
        "### 4. **No Need for Preprocessing (e.g., One-Hot Encoding)**\n",
        "\n",
        "* One of the primary advantages of **CatBoost** over other gradient boosting models is that it **does not require one-hot encoding** or other forms of explicit preprocessing for categorical features.\n",
        "* The model itself automatically handles categorical data internally, which greatly simplifies the data preparation process and saves both memory and computation time.\n",
        "\n",
        "### 5. **Efficient Memory Usage**\n",
        "\n",
        "* CatBoost handles categorical features without creating large, sparse matrices like one-hot encoding would. This helps keep the memory footprint small, especially when dealing with a large number of categorical variables or high-cardinality features.\n",
        "* Instead of creating many dummy variables, CatBoost just uses a compact numeric representation (based on target statistics) that is more memory-efficient.\n",
        "\n",
        "### 6. **Handling of Missing Values**\n",
        "\n",
        "* **CatBoost** is capable of efficiently handling missing values in both numerical and categorical features.\n",
        "* It uses **special algorithms** that help the model deal with missing data in categorical variables during training and prediction, preventing the need for complicated imputation techniques.\n",
        "\n",
        "### 7. **Fast Training Speed**\n",
        "\n",
        "* Thanks to its efficient handling of categorical features, CatBoost can train models faster compared to other gradient boosting algorithms that require extra processing steps for categorical data.\n",
        "* It also includes **multi-threading** and **GPU support**, which further enhance the speed of training, especially with large datasets.\n",
        "\n",
        "### 8. **Robust to Overfitting**\n",
        "\n",
        "* CatBoost includes techniques like **Random Permutation (shuffling)** and **Early Stopping** that further help in preventing overfitting.\n",
        "* It uses **gradient-based optimization** alongside robust regularization methods, making it resilient to noisy or small datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of CatBoost Handling Categorical Data:**\n",
        "\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset with categorical data\n",
        "data = {\n",
        "    'color': ['red', 'blue', 'green', 'red', 'blue'],\n",
        "    'size': ['small', 'large', 'small', 'large', 'small'],\n",
        "    'target': [1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert categorical columns into appropriate format for CatBoost\n",
        "cat_features = ['color', 'size']\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "\n",
        "# Train a CatBoost model\n",
        "model = CatBoostClassifier(cat_features=cat_features, iterations=100, learning_rate=0.1)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(predictions)\n",
        "```\n",
        "\n",
        "In this example:\n",
        "\n",
        "* **`cat_features`** parameter is used to specify the categorical columns. CatBoost internally handles their encoding without needing to apply one-hot encoding.\n",
        "* The model is fit directly with categorical features in their original form, simplifying the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of CatBoost's Efficiency in Handling Categorical Data**:\n",
        "\n",
        "1. **Ordered Target Statistics**: CatBoost calculates the target mean for each category based on past observations to avoid data leakage.\n",
        "2. **No Need for Manual Encoding**: It automatically handles categorical features, reducing preprocessing steps.\n",
        "3. **Efficient Memory Usage**: By using target-based encoding instead of one-hot encoding, it reduces dimensionality and memory overhead.\n",
        "4. **Handling High Cardinality**: CatBoost can efficiently manage categorical features with many unique values without creating sparse matrices.\n",
        "5. **Fast and Robust**: The model uses various optimizations to ensure fast training while avoiding overfitting.\n",
        "\n",
        "Overall, **CatBoost** is considered efficient for handling categorical data because of its built-in, well-optimized techniques that directly integrate categorical features into the model-building process without complex preprocessing."
      ],
      "metadata": {
        "id": "b73CthC_46Z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) What are some real-world applications where boosting techniques are preferred over bagging methods?"
      ],
      "metadata": {
        "id": "Pi4Oi1M35IDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting techniques are preferred over bagging methods in several real-world applications due to their ability to improve the predictive performance of weak models and their tendency to work well with **imbalanced data**, **complex patterns**, and **high variance**. Here are some examples of when boosting techniques, such as **AdaBoost**, **Gradient Boosting**, **XGBoost**, and **CatBoost**, are preferred over bagging methods like **Random Forest**:\n",
        "\n",
        "### 1. **Imbalanced Classification Problems**\n",
        "\n",
        "* **Scenario**: In applications where one class is significantly more frequent than another (e.g., fraud detection, rare event prediction), boosting is often preferred.\n",
        "* **Why Boosting Works Better**: Boosting methods like **XGBoost** and **AdaBoost** focus on misclassified examples, which is particularly useful when dealing with rare events or underrepresented classes. By assigning higher weights to misclassified instances, boosting helps the model focus on the minority class, thus improving overall performance on imbalanced datasets.\n",
        "* **Example**: **Credit card fraud detection** ‚Äì where fraudulent transactions are rare but critical to identify.\n",
        "\n",
        "### 2. **Predicting Complex, Non-linear Patterns**\n",
        "\n",
        "* **Scenario**: Boosting techniques are ideal for applications where the relationship between the features and the target variable is **non-linear** or highly complex, such as in **image classification**, **natural language processing (NLP)**, or **bioinformatics**.\n",
        "* **Why Boosting Works Better**: Boosting iteratively refines its predictions by focusing on errors from previous models. This ability to capture and adjust to complex patterns, especially with gradient boosting methods, gives it an edge over bagging methods, which work by averaging predictions of multiple independent models.\n",
        "* **Example**: **Image recognition and classification** ‚Äì Boosting methods like **Gradient Boosting** or **XGBoost** can learn complex image features, providing better results than bagging methods that rely on random subsets of data.\n",
        "\n",
        "### 3. **High-Variance Data**\n",
        "\n",
        "* **Scenario**: In cases where the model needs to be able to handle high variance in the data (i.e., data that is highly noisy or has many outliers), boosting is often preferred.\n",
        "* **Why Boosting Works Better**: Boosting **reduces variance** by systematically correcting errors, whereas bagging methods like Random Forest tend to focus on **averaging** many trees, which can result in higher variance when the trees are not strong enough.\n",
        "* **Example**: **Customer churn prediction** ‚Äì where there may be many variables interacting in non-obvious ways, boosting helps capture these interactions better.\n",
        "\n",
        "### 4. **Small to Medium-Sized Datasets**\n",
        "\n",
        "* **Scenario**: Boosting methods tend to work better on smaller to medium-sized datasets where the learning process can afford to focus on **refining mistakes** iteratively.\n",
        "* **Why Boosting Works Better**: Boosting methods like **XGBoost** or **LightGBM** are designed to be **more sample-efficient** compared to bagging methods like **Random Forest**, which require a large number of trees to provide stable predictions.\n",
        "* **Example**: **Healthcare data analysis** ‚Äì For predicting patient outcomes from relatively small datasets, boosting techniques can work better by improving model accuracy iteratively.\n",
        "\n",
        "### 5. **Feature Importance and Interpretability**\n",
        "\n",
        "* **Scenario**: Applications that require **interpretability** and **feature selection**, such as in **financial modeling** or **healthcare**, benefit from boosting methods, which provide better insights into feature importance.\n",
        "* **Why Boosting Works Better**: Boosting, especially algorithms like **XGBoost**, outputs a feature importance score, helping practitioners understand which features contribute most to the final predictions. This is often crucial in industries where transparency is necessary.\n",
        "* **Example**: **Risk assessment in insurance** ‚Äì Boosting can help determine which factors (e.g., age, driving history) are most predictive of a customer‚Äôs likelihood to file a claim.\n",
        "\n",
        "### 6. **Competition in Machine Learning (Kaggle Competitions)**\n",
        "\n",
        "* **Scenario**: Boosting methods are commonly used to win machine learning competitions, especially those hosted on platforms like Kaggle, where the goal is to achieve the highest predictive accuracy.\n",
        "* **Why Boosting Works Better**: Boosting models such as **XGBoost**, **LightGBM**, and **CatBoost** are known for their high performance and ability to **fine-tune predictions**. They can outperform other models, including bagging methods, because they iteratively correct errors and are highly customizable.\n",
        "* **Example**: **Kaggle Titanic competition** ‚Äì Boosting methods are often a go-to choice because they typically yield higher accuracy compared to bagging methods like Random Forest.\n",
        "\n",
        "### 7. **Anomaly Detection**\n",
        "\n",
        "* **Scenario**: In anomaly detection tasks where the goal is to detect rare patterns or outliers (e.g., fraud detection, network intrusion detection), boosting is often preferred.\n",
        "* **Why Boosting Works Better**: Boosting is focused on difficult-to-predict or misclassified data points (the outliers or anomalies), and thus it is well-suited to tasks that involve identifying unusual or abnormal behavior in datasets.\n",
        "* **Example**: **Network intrusion detection** ‚Äì where anomalies (e.g., hacking attempts) are rare but critical to identify quickly.\n",
        "\n",
        "### 8. **Regression Problems with Complex Dependencies**\n",
        "\n",
        "* **Scenario**: For regression tasks where the target variable has a complex, non-linear relationship with the input features, boosting tends to be more effective than bagging.\n",
        "* **Why Boosting Works Better**: Boosting builds the model incrementally, focusing on minimizing the residuals (errors) from previous iterations. This allows boosting methods to learn **complex dependencies** between input features and the target.\n",
        "* **Example**: **Real estate price prediction** ‚Äì where prices depend on a combination of factors such as location, size, amenities, etc., with complex, non-linear relationships.\n",
        "\n",
        "### 9. **Time Series Forecasting with Trends or Seasonality**\n",
        "\n",
        "* **Scenario**: When predicting time series data that exhibits **trends** or **seasonality** (e.g., predicting sales, stock prices), boosting methods can work better by capturing temporal patterns.\n",
        "* **Why Boosting Works Better**: Boosting methods can be adapted to sequential data (by using lagged features) and focus on the errors made at each time step. This makes boosting particularly useful for time series forecasting, where the model needs to understand **patterns over time**.\n",
        "* **Example**: **Stock market prediction** ‚Äì Boosting models, like **XGBoost** or **CatBoost**, are often used to predict stock prices or other financial metrics because of their ability to capture temporal dependencies and trends in historical data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**: When to Prefer Boosting over Bagging\n",
        "\n",
        "* **Imbalanced Data**: Boosting focuses on hard-to-classify instances (misclassified data), which is useful in class imbalance.\n",
        "* **Complex Non-linear Relationships**: Boosting excels at capturing complex interactions in data, which bagging methods like Random Forest may miss.\n",
        "* **High-Variance Data**: Boosting can help reduce variance by learning from the residuals, while bagging reduces variance by averaging predictions.\n",
        "* **Small to Medium Datasets**: Boosting works well with small datasets, especially when combined with regularization techniques.\n",
        "* **Feature Importance**: Boosting provides better feature importance, which is useful in interpretable modeling tasks.\n",
        "\n",
        "In contrast, **bagging methods like Random Forest** are generally better when:\n",
        "\n",
        "* You want a **robust** model with less focus on fine-tuning.\n",
        "* The data is **very noisy**, and you need to reduce variance by averaging many independent models.\n",
        "* You have **enough data** to handle the computational cost of training many trees.\n",
        "\n",
        "Both boosting and bagging have their place, but boosting is often preferred in scenarios where **high predictive accuracy**, **complex patterns**, and **fine-grained error correction** are critical."
      ],
      "metadata": {
        "id": "9RanPQyz5Ro1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets:\n",
        "## ‚óè Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "## ‚óè Use sklearn.datasets.fetch_california_housing() for regression tasks.\n",
        "\n",
        "## 6) Write a Python program to:\n",
        "\n",
        "## ‚óè Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "## ‚óè Print the model accuracy"
      ],
      "metadata": {
        "id": "xV5hCfZliV1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "ada_boost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "ada_boost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ada_boost.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "sdGznvm32LMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0459e9-781c-4660-9d5f-13e54eb5032e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 96.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Write a Python program to:\n",
        "\n",
        "## ‚óè Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "##‚óè Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "fWOzC_SQis9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the R-squared score of the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(f\"Gradient Boosting Regressor R-squared score: {r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxdrNcitinLh",
        "outputId": "54a8fc58-0888-47a7-c5e8-71f3b5fe516e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Write a Python program to:\n",
        "\n",
        "‚óè Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "‚óè Tune the learning rate using GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "qdvTrbtvi92r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for tuning the learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to tune the learning rate\n",
        "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score (accuracy)\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "# Print the best parameters and accuracy on training data\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Train the XGBoost model using the best parameters\n",
        "best_xgb = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy on the test set\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzSK_Dyni4Mi",
        "outputId": "7a4b547c-86a3-46a1-fb22-24d9c85df70c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "Best Parameters: {'learning_rate': 0.2, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy: 96.70%\n",
            "Test Accuracy: 95.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Write a Python program to:\n",
        "\n",
        "‚óè Train a CatBoost Classifier\n",
        "\n",
        "‚óè Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "BFgf2f4ujHNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=0)\n",
        "\n",
        "# Train the CatBoost model\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"CatBoost Classifier Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title(\"Confusion Matrix for CatBoost Classifier\")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "bUKOsZ-UjFfX",
        "outputId": "c60de03d-c179-416d-c1b3-15949bce63f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier Accuracy: 97.37%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "‚óè Data preprocessing & handling missing/categorical values\n",
        "\n",
        "‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "‚óè Hyperparameter tuning strategy\n",
        "\n",
        "‚óè Evaluation metrics you'd choose and why\n",
        "\n",
        "‚óè How the business would benefit from your model"
      ],
      "metadata": {
        "id": "SH6yAtXqjp_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Science Pipeline for Predicting Loan Default Using Boosting Techniques\n",
        "\n",
        "When working on a loan default prediction model in a FinTech company with imbalanced data, missing values, and a mix of numeric and categorical features, it‚Äôs important to follow a well-structured data science pipeline. Below is the step-by-step approach for building the model using boosting techniques like AdaBoost, XGBoost, or CatBoost.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Data Preprocessing & Handling Missing/Categorical Values**\n",
        "\n",
        "#### a. **Load and Inspect Data:**\n",
        "\n",
        "* **Initial Steps:** Load the dataset and inspect its structure, including dimensions, types of features, and the presence of missing values or outliers.\n",
        "\n",
        "  ```python\n",
        "  import pandas as pd\n",
        "  data = pd.read_csv('loan_default_data.csv')\n",
        "  data.info()\n",
        "  data.describe()\n",
        "  ```\n",
        "\n",
        "#### b. **Handling Missing Values:**\n",
        "\n",
        "* **Imputation Strategy:** If missing values are present, decide between imputing with the mean, median, or mode for numerical features and the mode for categorical features.\n",
        "\n",
        "  * For numerical features, use **median** (less sensitive to outliers):\n",
        "\n",
        "    ```python\n",
        "    data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())\n",
        "    ```\n",
        "  * For categorical features, use **mode**:\n",
        "\n",
        "    ```python\n",
        "    data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode()[0])\n",
        "    ```\n",
        "\n",
        "  If the percentage of missing values is very high in a feature, consider dropping it entirely.\n",
        "\n",
        "  ```python\n",
        "  data.dropna(axis=1, thresh=0.7*len(data), inplace=True)\n",
        "  ```\n",
        "\n",
        "#### c. **Handling Categorical Variables:**\n",
        "\n",
        "* **Encoding Categorical Variables:** Categorical features need to be encoded. There are two approaches:\n",
        "\n",
        "  * **One-Hot Encoding** for nominal categorical variables with fewer categories (e.g., gender).\n",
        "\n",
        "    ```python\n",
        "    data = pd.get_dummies(data, columns=['category_feature'])\n",
        "    ```\n",
        "  * **Label Encoding** for ordinal categorical variables or features with a large number of categories.\n",
        "\n",
        "    ```python\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    data['category_feature'] = le.fit_transform(data['category_feature'])\n",
        "    ```\n",
        "\n",
        "#### d. **Feature Scaling:**\n",
        "\n",
        "* **Standardization** is necessary for boosting algorithms when numerical features have different scales.\n",
        "\n",
        "  ```python\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
        "  ```\n",
        "\n",
        "#### e. **Addressing Imbalanced Data:**\n",
        "\n",
        "* **Resampling:** Since the dataset is imbalanced, we need to handle this problem.\n",
        "\n",
        "  * **Oversampling** (SMOTE or random oversampling) to balance the classes:\n",
        "\n",
        "    ```python\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    smote = SMOTE(sampling_strategy='minority')\n",
        "    X_res, y_res = smote.fit_resample(X, y)\n",
        "    ```\n",
        "  * **Undersampling** (random undersampling) can also be an option.\n",
        "  * Alternatively, **class weights** can be adjusted in the model (boosting algorithms like XGBoost and CatBoost can handle this natively).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Choice Between AdaBoost, XGBoost, or CatBoost**\n",
        "\n",
        "* **AdaBoost**:\n",
        "\n",
        "  * **Pros**: Simple, fast, works well with small datasets.\n",
        "  * **Cons**: Sensitive to noisy data, less powerful for large datasets or complex patterns compared to XGBoost and CatBoost.\n",
        "\n",
        "* **XGBoost**:\n",
        "\n",
        "  * **Pros**: Powerful and highly efficient for large datasets, handles missing values natively, supports regularization, and is robust to overfitting.\n",
        "  * **Cons**: Can be slower compared to other boosting techniques if the model is not optimized.\n",
        "\n",
        "* **CatBoost**:\n",
        "\n",
        "  * **Pros**: Specifically designed to handle categorical features efficiently without needing to manually encode them, often yields great results with minimal tuning.\n",
        "  * **Cons**: Can be more resource-intensive, especially for very large datasets.\n",
        "\n",
        "### **Recommendation**: Since the dataset contains both numeric and categorical features, **CatBoost** would likely perform well without needing heavy feature engineering. If computational resources are available, I would prioritize **CatBoost** for its ease of use and handling of categorical features.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Hyperparameter Tuning Strategy**\n",
        "\n",
        "* **Cross-validation:** Perform hyperparameter tuning using **GridSearchCV** or **RandomizedSearchCV** with k-fold cross-validation (usually 5 or 10 folds). This helps ensure the model generalizes well on unseen data.\n",
        "\n",
        "  Example with **CatBoost**:\n",
        "\n",
        "  ```python\n",
        "  from catboost import CatBoostClassifier\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "  param_grid = {\n",
        "      'iterations': [200, 500, 1000],\n",
        "      'depth': [4, 6, 8],\n",
        "      'learning_rate': [0.01, 0.05, 0.1],\n",
        "      'l2_leaf_reg': [1, 3, 5]\n",
        "  }\n",
        "\n",
        "  catboost_model = CatBoostClassifier(cat_features=categorical_columns)\n",
        "  grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=5, verbose=1, scoring='accuracy')\n",
        "  grid_search.fit(X_res, y_res)\n",
        "\n",
        "  best_params = grid_search.best_params_\n",
        "  print(f\"Best Parameters: {best_params}\")\n",
        "  ```\n",
        "\n",
        "* **Early Stopping:** Use early stopping during training to prevent overfitting. Many boosting algorithms like **XGBoost** and **CatBoost** support early stopping out of the box.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Evaluation Metrics**\n",
        "\n",
        "* **Accuracy**: While a commonly used metric, accuracy may not fully reflect the performance when the dataset is imbalanced.\n",
        "\n",
        "* **Precision, Recall, F1-Score**: These are more meaningful metrics in an imbalanced setting.\n",
        "\n",
        "  * **Precision**: The proportion of true positives out of all predicted positives. It‚Äôs useful to minimize false positives (e.g., incorrectly flagging a non-defaulting customer as likely to default).\n",
        "  * **Recall**: The proportion of true positives out of all actual positives. It‚Äôs useful when missing a true default (false negative) is more costly than a false positive.\n",
        "  * **F1-Score**: The harmonic mean of precision and recall. It balances both metrics, especially when dealing with class imbalance.\n",
        "\n",
        "  ```python\n",
        "  from sklearn.metrics import classification_report\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  ```\n",
        "\n",
        "* **ROC-AUC Score**: Since we‚Äôre dealing with imbalanced classes, **AUC-ROC** is useful to evaluate how well the model distinguishes between the classes.\n",
        "\n",
        "  ```python\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  auc = roc_auc_score(y_test, y_pred_prob)\n",
        "  print(f\"AUC-ROC Score: {auc}\")\n",
        "  ```\n",
        "\n",
        "* **Confusion Matrix**: Use a confusion matrix to visualize the number of false positives, false negatives, true positives, and true negatives.\n",
        "\n",
        "  ```python\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **How the Business Would Benefit from the Model**\n",
        "\n",
        "* **Risk Mitigation**: By accurately predicting loan defaults, the business can minimize losses by identifying high-risk customers in advance. The model can enable the company to implement stricter credit policies or offer risk-based pricing for loans.\n",
        "\n",
        "* **Operational Efficiency**: Automating the loan approval process and flagging high-risk customers for further review reduces the need for manual intervention, speeding up decision-making.\n",
        "\n",
        "* **Customer Segmentation**: The model can help segment customers into different risk categories. This allows for tailored communication and interventions (e.g., offering financial counseling or setting up payment plans for high-risk customers).\n",
        "\n",
        "* **Regulatory Compliance**: A reliable model that predicts defaults can ensure that the company remains compliant with industry regulations regarding responsible lending practices.\n",
        "\n",
        "* **Increased Profitability**: By reducing defaults, the company can improve its profitability while maintaining a competitive interest rate, attracting more customers without increasing financial risk.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "By following this pipeline, we can build an effective loan default prediction model that leverages the power of boosting techniques (like **CatBoost**), handles missing values and categorical features efficiently, performs hyperparameter tuning, and evaluates the model using appropriate metrics. The business can benefit significantly from better risk management, operational efficiencies, and improved profitability."
      ],
      "metadata": {
        "id": "ERoAOSd-j6LM"
      }
    }
  ]
}